{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\echow\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\librosa\\util\\decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\n",
      "  from numba.decorators import jit as optional_jit\n",
      "C:\\Users\\echow\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\librosa\\util\\decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\n",
      "  from numba.decorators import jit as optional_jit\n",
      "C:\\Users\\echow\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:70% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from more_itertools import peekable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as scp\n",
    "import pickle\n",
    "import magenta\n",
    "import os, time, re, json\n",
    "%matplotlib inline\n",
    "from IPython.core.display import display, HTML\n",
    "### change width of notebook display\n",
    "display(HTML(\"<style>.container { width:70% !important; }</style>\"))\n",
    "\n",
    "import plotly.express as px\n",
    "from jupyter_dash import JupyterDash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output\n",
    "\n",
    "# for exposing API\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output\n",
    "from flask import Flask\n",
    "from flask_restful import Resource, Api\n",
    "\n",
    "JUPYTER_PICKLE_FILE = \"config/shared_jupyter_data.pkl\"\n",
    "def write_shared_jupyter(key, value, path=JUPYTER_PICKLE_FILE, overwrite=False):\n",
    "    if (os.path.exists(path)):\n",
    "        with open(path, \"rb\") as fp:\n",
    "            shared_jupyter_data = pickle.load(fp)\n",
    "        if overwrite:\n",
    "            shared_jupyter_data = {key: value}\n",
    "        else:\n",
    "            shared_jupyter_data[key] = value\n",
    "    else:\n",
    "        shared_jupyter_data = {key: value}\n",
    "    with open(path, 'wb') as fp: \n",
    "        pickle.dump(shared_jupyter_data, fp)\n",
    "\n",
    "def read_shared_jupyter(key=None, path=JUPYTER_PICKLE_FILE):\n",
    "    if (os.path.exists(path)):\n",
    "        with open(path, \"rb\") as fp:\n",
    "            shared_jupyter_data = pickle.load(fp)\n",
    "            if key is not None:\n",
    "                if key in shared_jupyter_data:\n",
    "                    return(shared_jupyter_data[key])\n",
    "                else:\n",
    "                    print(\"Not found!\")\n",
    "                    return(None)\n",
    "            else:\n",
    "                return(shared_jupyter_data)\n",
    "    else:\n",
    "        print(\"No data\")\n",
    "\n",
    "def pandasToJson(df):\n",
    "    return(df.to_json(orient=\"split\"))\n",
    "def jsonToPandas(json):\n",
    "    return(pd.read_json(json, orient=\"split\"))\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "FIG_WIDTH = 1200\n",
    "FIG_HEIGHT = 800\n",
    "\n",
    "PITCH_MIN = 20\n",
    "PITCH_MAX = 120\n",
    "VELOCITY_MIN = 0\n",
    "VELOCITY_MAX = 120\n",
    "\n",
    "def hheader(x):\n",
    "    print(\"#########################################\")\n",
    "    print(\"### {}\".format(x))\n",
    "    print(\"#########################################\")\n",
    "\n",
    "# Magenta dependencies:\n",
    "# https://github.com/magenta/magenta\n",
    "\n",
    "# Magenta uses pretty_midi to deal with midi files\n",
    "import pretty_midi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in music stream and make decisions\n",
    "\n",
    "Each agent:\n",
    "- Read in music stream\n",
    "- Update predictive model\n",
    "- Probabilistic decision rule: create order and submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash is running on http://127.0.0.1:8122/\n",
      "\n",
      "Dash app running on http://127.0.0.1:8122/\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Each agent reads in music stream and responds\n",
    "    (start with a single agent)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" Kalman filter parameters \"\"\"\n",
    "\n",
    "from scipy.stats import multivariate_normal\n",
    "from statsmodels.tsa.stattools import acovf\n",
    "from scipy.ndimage.interpolation import shift\n",
    "np.random.seed(42)\n",
    "np.set_printoptions(suppress=True) \n",
    "\n",
    "def pandas_fill(arr):\n",
    "    df = pd.DataFrame(arr)\n",
    "    df = df.fillna(method='ffill', axis=1, inplace=False)\n",
    "    out = df.to_numpy()\n",
    "    return out\n",
    "\n",
    "# for now, keep the noise matrices as constants (can improve later)\n",
    "\n",
    "# observations x: note, different dimension from Z (unlike sample code)\n",
    "df_X = None\n",
    "x_obs = None\n",
    "[n,d] = [None, None]\n",
    "x_obs_nonna = None\n",
    "\n",
    "# number of lags (minus one)\n",
    "L = 14\n",
    "\n",
    "### Latent state equation - vector z_t\n",
    "# z_t = A_t z_{t-1} + <other regressors> + w_t\n",
    "At = np.eye(L) # (L x L) - paper says to leave this as identity (random walk)\n",
    "Qt = np.diag(np.ones(L)) # (L x L) covariance of errors in state eq. - TUNE THIS\n",
    "\n",
    "### Observation equation - scalar x_t\n",
    "# x_t = C_t z_t + <other regressors> + v_t\n",
    "Ct = np.ones((1,L)) # (1 x L) - ARIMA lags - WILL OVERRIDE\n",
    "Rt = np.diag(np.ones(1)) # (1 x 1)\n",
    "\n",
    "# Simulate some plausible values for Qt and Rt\n",
    "# (could make Rt self-updating if time)\n",
    "Qt = np.diag(np.random.gamma(1,0.1,size=L)) # covar. mat is symmetric\n",
    "Rt = np.random.gamma(3, 4, size=np.diag(np.ones(1)).shape)\n",
    "\n",
    "# Initial conditions (for the state variable z, stored in mu vars)\n",
    "# z0 = x_obs[1:(L+1)]\n",
    "# initialize to equal weight on all lags\n",
    "z0 = np.repeat(1/L, L).reshape(-1, 1) # make state variable the WEIGHTS on lags NOT the LAGS themselves\n",
    "P0 = Qt\n",
    "\n",
    "#Objects to store predictions and filtering locations\n",
    "Z = None\n",
    "Zpred = None\n",
    "Xpred = None\n",
    "\n",
    "# store predictions\n",
    "z = z0 # predicted zhat, ()\n",
    "P = P0 # covariance of zhat, dim(L, L)\n",
    "Ct = Ct0 = None\n",
    "\n",
    "\"\"\" Iteration parameters \"\"\"\n",
    "\n",
    "from collections import OrderedDict\n",
    "import requests\n",
    "\n",
    "### Iteration parameters\n",
    "STREAM_SLEEP_SEC = 1\n",
    "\n",
    "\"\"\" Agent parameters \"\"\"\n",
    "WEALTH = 10000\n",
    "\n",
    "\"\"\" Plotly parameters \"\"\"\n",
    "# keep it simple and update every iteration\n",
    "DASH_PORT = 8122\n",
    "PLOTLY_REFRESH_SEC = 0.5\n",
    "PLOTLY_STREAM_SLEEP_SEC = PLOTLY_REFRESH_SEC * 5\n",
    "\n",
    "### Visualize all historical data\n",
    "server = Flask('my_app')\n",
    "external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\n",
    "app = JupyterDash(__name__, server=server, external_stylesheets=external_stylesheets)\n",
    "### use later for sending trade signals\n",
    "#     api = Api(server)\n",
    "#     class vizStreamSoFar(Resource):\n",
    "#         \"\"\" Get current snapshot of the music stream.\n",
    "#             $ curl \"http://localhost:8122/read\"\n",
    "#         \"\"\"\n",
    "#         def get(self):\n",
    "#             return({\"currTime\": currTime, \"lastBar\": pandasToJson(lastBar)})\n",
    "#     api.add_resource(vizStreamSoFar, '/read')\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"Agent sample statistics\"),\n",
    "    dcc.Graph(id='pitch_graph'),\n",
    "    dcc.Interval(id='interval-component', interval=PLOTLY_REFRESH_SEC*1000, n_intervals=0)\n",
    "# ])\n",
    "], style={'width': '80%', 'float': 'left', 'height': '4.5rem'})\n",
    "        \n",
    "@app.callback(Output('pitch_graph', 'figure'), Input(\"interval-component\", \"n_intervals\"))\n",
    "def update_pitch_figure(n=0):\n",
    "    \"\"\"\n",
    "    Update plotly figure. (Like ggplot2: color based on group)\n",
    "    currBar (global): variable with the current data.\n",
    "    \"\"\"\n",
    "    fig = px.line(df_preds, x=\"streaming_start_sec\",\n",
    "                  y=['raw', \"preds\"],\n",
    "                  render_mode=\"webgl\", template=\"plotly_dark\",\n",
    "        title=\"Observed pitch statistics\",range_y=[PITCH_MIN, PITCH_MAX]).update_traces(mode='markers+lines')\n",
    "#     fig = px.line(assetDataSoFar_df, x=\"streaming_start_sec\",\n",
    "#                   y=['pitch_mean'],\n",
    "#                   render_mode=\"webgl\", template=\"plotly_dark\",\n",
    "#         title=\"Observed pitch statistics\",range_y=[PITCH_MIN, PITCH_MAX]).update_traces(mode='markers+lines')\n",
    "    return(fig)\n",
    "\n",
    "### Run app locally (inline cuts off output)\n",
    "app.run_server(mode='external', port=DASH_PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 ...\n",
      "Iteration 2 ...\n",
      "Iteration 3 ...\n",
      "Iteration 4 ...\n",
      "Iteration 5 ...\n",
      "Observation stored\n",
      "Observation stored\n",
      "Observation stored\n",
      "Observation stored\n",
      "Observation stored\n",
      "Iteration 6 ...\n",
      "Iteration 7 ...\n",
      "Iteration 8 ...\n",
      "Iteration 9 ...\n",
      "Iteration 10 ...\n",
      "Observation stored\n",
      "Observation stored\n",
      "Observation stored\n",
      "Observation stored\n",
      "Observation stored\n",
      ">>> initializing model and training on historical data until current ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 11 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 12 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 13 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 14 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 15 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 16 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 17 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 18 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 19 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 20 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 21 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 22 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 23 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 24 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 25 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 26 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 27 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 28 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 29 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 30 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 31 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 32 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 33 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 34 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 35 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 36 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 37 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 38 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 39 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 40 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 41 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 42 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 43 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 44 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 45 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 46 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 47 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 48 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 49 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 50 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 51 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 52 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 53 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 54 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 55 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 56 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 57 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 58 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 59 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 60 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 61 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 62 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 63 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 64 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 65 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 66 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 67 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 68 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 69 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 70 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 71 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 72 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 73 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 74 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 75 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 76 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 77 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 78 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 79 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 80 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 81 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 82 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 83 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 84 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 85 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 86 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 87 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 88 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 89 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 90 ...\n",
      ">>> updating model on current bar of data ...\n",
      "Iteration 91 ...\n",
      ">>> updating model on current bar of data ...\n"
     ]
    }
   ],
   "source": [
    "# Iteration = whether to ping for more data\n",
    "# extra level of sampling on top of the raw music stream (which itself is sampled)\n",
    "iterations = 1\n",
    "assetDataSoFar = OrderedDict()\n",
    "assetDataSoFar_df = None # always-concatenated copy of all historical data\n",
    "modelInitialized = False\n",
    "MAX_ARRAY_LEN = 10**4\n",
    "while True:\n",
    "    print(\"Iteration {} ...\".format(iterations))\n",
    "    \n",
    "    ### Read in music stream (assume perpetual)\n",
    "    currAssetData = requests.get(\"http://localhost:8100/read\")\n",
    "    if (currAssetData.status_code != 200):\n",
    "        raise Exception(\"GET request failed.\")\n",
    "    currAsset = currAssetData.json()\n",
    "    currAssetTime = currAsset['currTime']\n",
    "    currAssetBar = jsonToPandas(currAsset['lastBar'])\n",
    "    \n",
    "    ### add to historical data if hasn't been seen yet\n",
    "    for sx in range(currAssetBar.shape[0]):\n",
    "        currAssetObs = currAssetBar.iloc[[sx]]\n",
    "        currStreamingIx = currAssetObs.streaming_start_sec.values[0]\n",
    "        if (currStreamingIx not in assetDataSoFar):\n",
    "            assetDataSoFar[currStreamingIx] = currAssetObs\n",
    "        else:\n",
    "            print(\"Observation stored\")\n",
    "    \n",
    "    ### concatenate all historical data so far (only if does not exist)\n",
    "    assetDataSoFar_df = pd.concat(assetDataSoFar.values())\n",
    "\n",
    "    ### Need enough data, otherwise won't have enough \n",
    "    ### to use lags (for Kalman model) or train historically (generally).\n",
    "    if (iterations < 10):\n",
    "        iterations += 1\n",
    "        continue\n",
    "        \n",
    "    \"\"\" MODEL TRAINING AND ANALYSIS #######################################################\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Define data so far, with and without current bar\n",
    "    ### TODO(echow): refactor to make faster\n",
    "    currIxs = assetDataSoFar_df.streaming_start_sec.isin(currAssetBar.streaming_start_sec)\n",
    "    assetDataSoFar_withCurr_df = assetDataSoFar_df\n",
    "    assetDataSoFar_noCurr_df = assetDataSoFar_df.loc[~currIxs]\n",
    "    assetDataSoFar_onlyCurr_df = assetDataSoFar_df.loc[currIxs] # same as currAssetBarNotSeen\n",
    "    \n",
    "    ### if model is not already trained, train model\n",
    "    if not modelInitialized:\n",
    "        print(\">>> initializing model and training on historical data until current ...\")\n",
    "        ######## Initialize model\n",
    "        # observations x: note, different dimension from Z (unlike sample code)\n",
    "        df_X = assetDataSoFar_noCurr_df[['pitch_mean']]\n",
    "        x_obs = np.c_[df_X]\n",
    "        [n,d] = x_obs.shape\n",
    "        # create version of observations forward-filled\n",
    "        x_obs_nonna = x_obs.copy()\n",
    "        x_obs_nonna = pd.DataFrame(x_obs_nonna).ffill()[0].to_numpy().reshape(-1, 1)\n",
    "        #Objects to store predictions and filtering locations\n",
    "        Z = np.zeros((MAX_ARRAY_LEN,L))\n",
    "        Zpred = np.zeros((MAX_ARRAY_LEN,L))\n",
    "        Xpred = np.zeros((MAX_ARRAY_LEN,1))\n",
    "        Ct0 = x_obs_nonna[0:(L)].T\n",
    "        # fill forward if NAs at the start (edge case)\n",
    "        if np.isnan(Ct0).any():\n",
    "            Ct0 = pandas_fill(Ct0)\n",
    "        Ct = Ct0\n",
    "        \n",
    "        ######## Train predictive model on historical data so far (will take some time)\n",
    "        timerange = range(L, n-1) # data starts at 0 so start analysis at L for L lags\n",
    "        for i in timerange:\n",
    "            ### Get current observations xt, t\n",
    "            # https://stats.stackexchange.com/questions/140990/using-kalman-filters-to-impute-missing-values-in-time-series\n",
    "            x = np.array(x_obs[i])\n",
    "            missing = any(np.isnan(x))\n",
    "            ### Prediction step using previous data against new data ---------------------------------------\n",
    "            # zhat, t|t-1\n",
    "            z = At.dot(z)\n",
    "            # Phat, t|t-1\n",
    "            P = At.dot(P).dot(At.T) + Qt\n",
    "            Zpred[i,:] = z.T\n",
    "            xhat = Ct.dot(z)\n",
    "            Xpred[i,:] = xhat\n",
    "            ### Measurement update incorporating new data ---------------------------------------------------\n",
    "            # handle missing\n",
    "            if (missing):\n",
    "                Z[i,:] = np.nan\n",
    "                continue\n",
    "            ### embed ARIMA within the Kalman filter\n",
    "            Ct = x_obs_nonna[(i-L):(i)].T\n",
    "            ### Calculate Kalman gain and update log-likehoo\n",
    "            # Kalman gain Kt\n",
    "            S = Ct.dot(P).dot(Ct.T) + Rt\n",
    "            Kt = P.dot(Ct.T).dot( np.linalg.inv(S))\n",
    "            ### Measurement update step\n",
    "            z = z + Kt.dot(x - Ct.dot(z))\n",
    "            Z[i,:] = z.T\n",
    "            # update P, t|t\n",
    "            P = P - Kt.dot(Ct).dot(P)\n",
    "        modelInitialized = True\n",
    "    \n",
    "    \"\"\" Train on current bar of data \"\"\"\n",
    "    print(\">>> updating model on current bar of data ...\")\n",
    "    # observations x: note, different dimension from Z (unlike sample code)\n",
    "    df_X = assetDataSoFar_withCurr_df[['pitch_mean']]\n",
    "    x_obs = np.c_[df_X]\n",
    "    [n,d] = x_obs.shape\n",
    "    Ct0 = x_obs_nonna[0:(L)].T\n",
    "    # create version of observations forward-filled\n",
    "    x_obs_nonna = x_obs.copy()\n",
    "    x_obs_nonna = pd.DataFrame(x_obs_nonna).ffill()[0].to_numpy().reshape(-1, 1)\n",
    "    # fill forward if NAs at the start (edge case)\n",
    "    if np.isnan(Ct0).any():\n",
    "        Ct0 = pandas_fill(Ct0)\n",
    "    Ct = Ct0\n",
    "    ### Update model on current bar of data (that hasn't been seen so far)\n",
    "    ### Start at the index of the current bar which has already been appended to the historical series.\n",
    "    currAssetBarTimeRange = np.where(assetDataSoFar_withCurr_df.streaming_start_sec.isin(currAssetBar.streaming_start_sec))[0]\n",
    "    for j in currAssetBarTimeRange:\n",
    "        ### Get current observations xt, t\n",
    "        # https://stats.stackexchange.com/questions/140990/using-kalman-filters-to-impute-missing-values-in-time-series\n",
    "        x = np.array(x_obs[j])\n",
    "        missing = any(np.isnan(x))\n",
    "        ### Prediction step using previous data against new data ---------------------------------------\n",
    "        # zhat, t|t-1\n",
    "        z = At.dot(z)\n",
    "        # Phat, t|t-1\n",
    "        P = At.dot(P).dot(At.T) + Qt\n",
    "        Zpred[j,:] = z.T\n",
    "        xhat = Ct.dot(z)\n",
    "        Xpred[j,:] = xhat\n",
    "        ### Measurement update incorporating new data ---------------------------------------------------\n",
    "        # handle missing\n",
    "        if (missing):\n",
    "            Z[j,:] = np.nan\n",
    "            continue\n",
    "        ### embed ARIMA within the Kalman filter\n",
    "        Ct = x_obs_nonna[(j-L):(j)].T\n",
    "        ### Calculate Kalman gain and update log-likehoo\n",
    "        # Kalman gain Kt\n",
    "        S = Ct.dot(P).dot(Ct.T) + Rt\n",
    "        Kt = P.dot(Ct.T).dot( np.linalg.inv(S))\n",
    "        ### Measurement update step\n",
    "        z = z + Kt.dot(x - Ct.dot(z))\n",
    "        Z[j,:] = z.T\n",
    "        # update P, t|t\n",
    "        P = P - Kt.dot(Ct).dot(P)\n",
    "    \n",
    "    ### Update dataframe of predictions\n",
    "    df_preds = pd.concat([\n",
    "        assetDataSoFar_df[[\"streaming_start_sec\"]].reset_index(drop=True),\n",
    "        assetDataSoFar_df[[\"pitch_mean\"]].reset_index(drop=True),\n",
    "        pd.DataFrame(Xpred[range(assetDataSoFar_df.shape[0]),:], columns=['pred'])\n",
    "    ], axis=1)\n",
    "    df_preds.columns = ['streaming_start_sec', 'raw', 'preds']\n",
    "\n",
    "    ### Take a short break between analyses (so plotly can catch up)\n",
    "    ### should be >> plot auto-update interval so that all plots\n",
    "    ### update basically at the same time. \n",
    "    time.sleep(STREAM_SLEEP_SEC) # 1 second is comfortable for nice UI\n",
    "    iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # t=timerange # start from when have enough lags\n",
    "# # t=range(0, n-1) # plot full time range\n",
    "# t = range(0, assetDataSoFar_df.shape[0])\n",
    "\n",
    "# ### Plot raw vs. predictions (on separate plots)\n",
    "# df_plt = pd.DataFrame({\n",
    "#     \"time\": t,\n",
    "#     \"raw\": x_obs[t, 0],\n",
    "#     \"pred\": Xpred[t, 0]\n",
    "# #     \"predicted\": Zpred[timerange,0],\n",
    "# #     \"filtered\": Z[timerange, 0]\n",
    "# })\n",
    "# df_plt.head()\n",
    "# fig1 = px.line(df_plt, x=\"time\", y=[\"raw\"], template=\"plotly_dark\",range_y=[0, PITCH_MAX])\n",
    "# fig2 = px.line(df_plt, x=\"time\", y=['pred'], template=\"plotly_dark\",range_y=[0, PITCH_MAX])\n",
    "# fig3 = px.line(df_plt, x=\"time\", y=[\"raw\", 'pred'], template=\"plotly_dark\",range_y=[0, PITCH_MAX])\n",
    "\n",
    "# ### Plot prediction errors\n",
    "# df_plt = pd.DataFrame({\n",
    "#     \"time\": t,\n",
    "#     \"error\": (x_obs[t, 0] - Xpred[t, 0]),\n",
    "#     \"error_sqr\": (x_obs[t,0] - Xpred[t, 0])**2\n",
    "# })\n",
    "# df_plt.head()\n",
    "# fig4 = px.line(df_plt, x=\"time\", y=[\"error\"], template=\"plotly_dark\")\n",
    "\n",
    "# ### Plot Kalman-estimated coefficient paths (coefs on lags)\n",
    "# df_coefs = pd.DataFrame(Zpred)\n",
    "# df_coefs.columns = [\"l{}\".format(l) for l in range(df_coefs.shape[1])]\n",
    "# df_coefs = df_coefs.reset_index()\n",
    "\n",
    "# fig5 = px.line(df_coefs.loc[df_coefs['index'].isin(t),:], x=\"index\",\n",
    "#                y=[c for c in df_coefs.columns if c != \"index\"], template=\"plotly_dark\")\n",
    "\n",
    "# print(\"Error stats for {} lags: \")\n",
    "# print(\"MSE: {}\\n\\n\".format(df_plt[['error_sqr']].mean()))\n",
    "# print(df_plt[['error']].describe())\n",
    "# # display(fig1)\n",
    "# # display(fig2)\n",
    "# display(fig3)\n",
    "# display(fig4)\n",
    "# display(fig5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offline results: analysis on the whole time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assetDataSoFar_df = pd.concat(assetDataSoFar.values())\n",
    "display(assetDataSoFar_df)\n",
    "display(currAssetBar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "plot time series of interest\n",
    "\"\"\"\n",
    "import seaborn as sns\n",
    "sns.lineplot(data=assetDataSoFar_df, x=\"streaming_start_sec\", y=\"pitch_mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "OFFLINE NEED TO MOVE ONLINE\n",
    "\n",
    "Find structural breaks: Kalman filter + CUSUM or some other online test\n",
    "Inspo: file:///C:/Users/echow/AppData/Local/Temp/remotesensing-12-03135-v2.pdf\n",
    "\"\"\"\n",
    "from scipy.stats import multivariate_normal\n",
    "from statsmodels.tsa.stattools import acovf\n",
    "from scipy.ndimage.interpolation import shift\n",
    "np.random.seed(42)\n",
    "\n",
    "np.set_printoptions(suppress=True) \n",
    "\n",
    "def pandas_fill(arr):\n",
    "    df = pd.DataFrame(arr)\n",
    "    df = df.fillna(method='ffill', axis=1, inplace=False)\n",
    "    out = df.to_numpy()\n",
    "    return out\n",
    "\n",
    "# for now, keep the noise matrices as constants (can improve later)\n",
    "\n",
    "# observations x: note, different dimension from Z (unlike sample code)\n",
    "df_X = assetDataSoFar_df[['pitch_mean']]\n",
    "x_obs = np.c_[df_X]\n",
    "[n,d] = x_obs.shape\n",
    "\n",
    "# create version of observations forward-filled\n",
    "x_obs_nonna = x_obs.copy()\n",
    "x_obs_nonna = pd.DataFrame(x_obs_nonna).ffill()[0].to_numpy().reshape(-1, 1)\n",
    "\n",
    "# number of lags (minus one)\n",
    "L = 14\n",
    "\n",
    "### Latent state equation - vector z_t\n",
    "# z_t = A_t z_{t-1} + <other regressors> + w_t\n",
    "At = np.eye(L) # (L x L) - paper says to leave this as identity (random walk)\n",
    "Qt = np.diag(np.ones(L)) # (L x L) covariance of errors in state eq. - TUNE THIS\n",
    "\n",
    "### Observation equation - scalar x_t\n",
    "# x_t = C_t z_t + <other regressors> + v_t\n",
    "Ct = np.ones((1,L)) # (1 x L) - ARIMA lags - WILL OVERRIDE\n",
    "Rt = np.diag(np.ones(1)) # (1 x 1)\n",
    "\n",
    "# df_x_obs_nonna_lags = []\n",
    "# for l in range(0, L):\n",
    "#     curr = shift(x_obs_nonna.reshape(-1,), l, cval=np.nan)\n",
    "#     df_curr = pd.DataFrame(curr, columns=[\"l{}\".format(l)])\n",
    "#     df_x_obs_nonna_lags.append(df_curr)\n",
    "# df_x_obs_nonna_lags = pd.concat(df_x_obs_nonna_lags, axis=1).dropna()\n",
    "# cov_mat_lags = df_x_obs_nonna_lags.cov()\n",
    "\n",
    "# # Tune Qt with historical covariances\n",
    "# Qt = cov_mat_lags\n",
    "\n",
    "# Simulate some plausible values for Qt and Rt\n",
    "# (could make Rt self-updating if time)\n",
    "Qt = np.diag(np.random.gamma(1,0.1,size=L)) # needs to be symmetric to be a proper covariance matrix\n",
    "Rt = np.random.gamma(3, 4, size=np.diag(np.ones(1)).shape)\n",
    "# At, Qt, Ct, Rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initial conditions (for the state variable z, stored in mu vars)\n",
    "# z0 = x_obs[1:(L+1)]\n",
    "z0 = np.repeat(1/L, L).reshape(-1, 1) # make state variable the WEIGHTS on lags NOT the LAGS themselves\n",
    "P0 = Qt\n",
    "\n",
    "#Objects to store predictions and filtering locations\n",
    "Z = np.zeros((n,L))\n",
    "Zpred = np.zeros((n,L))\n",
    "Xpred = np.zeros((n,L))\n",
    "\n",
    "# store predictions\n",
    "z = z0 # predicted zhat, ()\n",
    "P = P0 # covariance of zhat, dim(L, L)\n",
    "Ct0 = x_obs_nonna[0:(L)].T\n",
    "# fill forward if NAs at the start (edge case)\n",
    "if np.isnan(Ct).any():\n",
    "    Ct0 = pandas_fill(Ct0)\n",
    "Ct = Ct0\n",
    "\n",
    "timerange = range(L, n-1) # data starts at 0 so start analysis at L for L lags\n",
    "for i in timerange:\n",
    "        \n",
    "    ### Get current observations xt, t\n",
    "    # if missing value NA (resulting in missing Kalman predictions) then\n",
    "    # just run the prediction step and continue\n",
    "    # https://stats.stackexchange.com/questions/140990/using-kalman-filters-to-impute-missing-values-in-time-series\n",
    "    x = np.array(x_obs[i])\n",
    "    missing = any(np.isnan(x))\n",
    "    \n",
    "    ### Prediction step using previous data against new data ---------------------------------------\n",
    "    ### (can update variables directly bc don't require t-1|t-1 thereafter)\n",
    "    # zhat, t|t-1\n",
    "    z = At.dot(z)\n",
    "    # Phat, t|t-1\n",
    "    P = At.dot(P).dot(At.T) + Qt\n",
    "    Zpred[i,:] = z.T\n",
    "    xhat = Ct.dot(z)\n",
    "    Xpred[i,:] = xhat\n",
    "    \n",
    "    if not missing:\n",
    "        print(\"Predicted: {:5f}. Actual: {:5f}. (Error: {:5f})\".format(float(x), float(xhat), float(x - xhat)))\n",
    "    else:\n",
    "        print(\"... NaN value in x data, this is normal. Continue.\")\n",
    "    \n",
    "    ### Implement CUSUM for change point detection - later\n",
    "    ### (use low p-value and update every time)\n",
    "    \n",
    "    ### Measurement update incorporating new data ---------------------------------------------------\n",
    "    \n",
    "    # handle missing\n",
    "    if (missing):\n",
    "        Z[i,:] = np.nan\n",
    "        continue\n",
    "    \n",
    "    ### embed ARIMA within the Kalman filter\n",
    "    Ct = x_obs_nonna[(i-L):(i)].T\n",
    "\n",
    "    ### Calculate Kalman gain and update log-likehoo\n",
    "    # Kalman gain Kt\n",
    "    S = Ct.dot(P).dot(Ct.T) + Rt\n",
    "    Kt = P.dot(Ct.T).dot( np.linalg.inv(S))\n",
    "    # update log-likelihood\n",
    "    \n",
    "    ### Measurement update step\n",
    "    # update zhat, t\n",
    "    # this Ct dot Z is just way too big, i.e. doesn't estimate x well.\n",
    "    # So \n",
    "    z = z + Kt.dot(x - Ct.dot(z))\n",
    "    Z[i,:] = z.T\n",
    "    # update P, t|t\n",
    "    P = P - Kt.dot(Ct).dot(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Plot predictions\n",
    "t=timerange # start from when have enough lags\n",
    "# t=range(0, n-1) # plot full time range\n",
    "\n",
    "df_plt = pd.DataFrame({\n",
    "    \"time\": t,\n",
    "    \"raw\": x_obs[t, 0],\n",
    "    \"pred\": Xpred[t, 0]\n",
    "#     \"predicted\": Zpred[timerange,0],\n",
    "#     \"filtered\": Z[timerange, 0]\n",
    "})\n",
    "df_plt.head()\n",
    "fig1 = px.line(df_plt, x=\"time\", y=[\"raw\", \"pred\"], template=\"plotly_dark\")\n",
    "\n",
    "### Plot prediction errors\n",
    "df_plt = pd.DataFrame({\n",
    "    \"time\": t,\n",
    "    \"error\": (x_obs[t, 0] - Xpred[t, 0]),\n",
    "    \"error_sqr\": (x_obs[t,0] - Xpred[t, 0])**2\n",
    "})\n",
    "df_plt.head()\n",
    "fig2 = px.line(df_plt, x=\"time\", y=[\"error\"], template=\"plotly_dark\")\n",
    "\n",
    "### Plot Kalman-estimated coefficient paths (coefs on lags)\n",
    "df_coefs = pd.DataFrame(Zpred)\n",
    "df_coefs.columns = [\"l{}\".format(l) for l in range(df_coefs.shape[1])]\n",
    "df_coefs = df_coefs.reset_index()\n",
    "\n",
    "fig3 = px.line(df_coefs, x=\"index\",\n",
    "               y=[c for c in df_coefs.columns if c != \"index\"], template=\"plotly_dark\")\n",
    "\n",
    "print(\"Error stats for {} lags: \")\n",
    "print(\"MSE: {}\\n\\n\".format(df_plt[['error_sqr']].mean()))\n",
    "print(df_plt[['error']].describe())\n",
    "display(fig1)\n",
    "display(fig2)\n",
    "display(fig3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "start with one single time series - e.g. trading pitch_mean\n",
    "build Bayesian forecasting model based on that pitch_mean\n",
    "build trading rules on top of that, e.g. if think will go up with high confidence or rather rapidly, then submit buy signal\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
